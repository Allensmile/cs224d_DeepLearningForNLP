{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--TensorFlow tutorial for CS224D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start....\n",
      "dot_result: Tensor(\"MatMul:0\", shape=(2, 2), dtype=float32)\n",
      "a: Tensor(\"zeros:0\", shape=(2, 2), dtype=float32) b: Tensor(\"ones:0\", shape=(2, 2), dtype=float32)\n",
      "sum: [ 2.  2.]\n",
      "a_shape: (2, 2)\n",
      "a2.shape: (1, 4) a2: [[ 0.  0.  0.  0.]]\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print \"start....\"\n",
    "#create constant,a*b,sum,shape,reshape\n",
    "\n",
    "#set a default session.\n",
    "sess=tf.InteractiveSession() \n",
    "sess.as_default()\n",
    "a=tf.zeros((2,2));b=tf.ones((2,2))\n",
    "\n",
    "dot_result=tf.matmul(a,b) #.eval()\n",
    "print \"dot_result:\",dot_result\n",
    "\n",
    "print \"a:\",a,\"b:\",b\n",
    "sum=tf.reduce_sum(b,reduction_indices=1).eval() #np.sum(a, axis=1)\n",
    "print \"sum:\",sum\n",
    "\n",
    "a_shape=a.get_shape() #a.shape()\n",
    "print \"a_shape:\",a_shape \n",
    "\n",
    "a2=tf.reshape(a,(1,4)).eval() #np.reshape\n",
    "print \"a2.shape:\",a2.shape,\"a2:\",a2\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start....\n",
      "30.0\n",
      "30.0\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Session\n",
    "print \"start....\"\n",
    "a=tf.constant(5.0)\n",
    "b=tf.constant(6.0)\n",
    "c=a*b\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(c) # equals to c.eval()\n",
    "    print c.eval()\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start....\n",
      "W1: [[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "W2: [[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "print \"start....\"\n",
    "W1=tf.ones((2,2))\n",
    "W2=tf.Variable(tf.zeros((2,2)),name=\"weights\")\n",
    "with tf.Session() as sess:\n",
    "    print \"W1:\",sess.run(W1) \n",
    "    sess.run(tf.initialize_all_variables()) #init variables before run it.\n",
    "    print \"W2:\",sess.run(W2)\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "state.before add: 0\n",
      "state: 1\n",
      "state: 2\n",
      "state: 3\n",
      "state: 4\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#example of something like 'counter'\n",
    "print \"start...\"\n",
    "state=tf.Variable(0,name=\"counter\") #state=0. declare a variable 'counter'\n",
    "new_value=tf.add(state,tf.constant(1)) #new_value=state+1. add 'counter' with 1 to get a new value\n",
    "update=tf.assign(state,new_value) #update=new_value\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print \"state.before add:\",sess.run(state) \n",
    "    for _ in range(4):\n",
    "        sess.run(update) #state=state+1\n",
    "        print \"state:\",sess.run(state)\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "variables: [7.0, 21.0]\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Fetching Variable State\n",
    "print \"start...\"\n",
    "input1=tf.constant(3.0)\n",
    "input2=tf.constant(2.0)\n",
    "input3=tf.constant(5.0)\n",
    "intermed=tf.add(input2,input3)\n",
    "mul=tf.mul(intermed,input1)\n",
    "with tf.Session() as sess:\n",
    "    print \"variables:\",sess.run([intermed,mul])\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta: [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Input Data\n",
    "import numpy as np\n",
    "a=np.zeros((3,3)) #a is a numpy array\n",
    "ta=tf.convert_to_tensor(a) #convert numpy array to tensor\n",
    "with tf.Session() as sess:\n",
    "    print \"ta:\",sess.run(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#PlaceHoders: dummy variable just act as place holder\n",
    "input1=tf.placeholder(tf.float32)\n",
    "input2=tf.placeholder(tf.float32)\n",
    "output=tf.mul(input1,input2)\n",
    "with tf.Session() as sess:\n",
    "    print \"output:\",sess.run([output],feed_dict={input1:[7.],input2:[2.]}) #feed_dict to set value for input1 and input2. then calcuate output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Variable Scope\n",
    "print \"start...\"\n",
    "with tf.variable_scope(\"foo\"): #,reuse=True\n",
    "    with tf.variable_scope(\"bar\"): #,reuse=True\n",
    "        v=tf.get_variable(\"v\",[1])\n",
    "assert v.name==\"foo/bar/v:0\"\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Vraible Scope: reuse variables1\n",
    "print \"start...\"\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v=tf.get_variable(\"v\",[1]) #get one variable named \"v\"\n",
    "    tf.get_variable_scope().reuse_variables() #set variable reuse strategy\n",
    "    v1=tf.get_variable(\"v\",[1]) #get \"v\" again. since we set variable reuse strategy. it will get same variables\n",
    "assert v1==v\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Vraible Scope: reuse variables\n",
    "print \"start...\"\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    w=tf.get_variable(\"w\",[1])\n",
    "    \n",
    "with tf.variable_scope(\"foo\",reuse=True):\n",
    "    w=tf.get_variable(\"w\",[1])\n",
    "    w1=tf.get_variable(\"w\",[1])\n",
    "assert w==w1\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe1ef4412d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1ef4a0690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Linear Regression in TensorFlow.1:Prepare training data(X,y)\n",
    "import numpy as np\n",
    "#import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Define input data\n",
    "X_data=np.arange(100,step=.1)\n",
    "y_data=X_data+20*np.sin(X_data/10)\n",
    "\n",
    "#plot input data\n",
    "plt.scatter(X_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression in TensorFlow.2.Define input for batch operation later.\n",
    "n_samples=1000\n",
    "batch_size=100\n",
    "\n",
    "X_data=tf.reshape(X_data,(n_samples,1)) #source dataset\n",
    "y_data=tf.reshape(y_data,(n_samples,1)) #source label\n",
    "X_data = tf.cast(X_data, tf.float32)\n",
    "y_data = tf.cast(y_data, tf.float32)\n",
    "\n",
    "#Define placeholders for input\n",
    "X=tf.placeholder(tf.float32,shape=(batch_size,1)) #for batch use\n",
    "y=tf.placeholder(tf.float32,shape=(batch_size,1)) #for batch use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start....\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression in TensorFlow.3.Predict and compute Loss\n",
    "#Define variables to be learned\n",
    "print \"start....\"\n",
    "with tf.variable_scope(\"linear-regression10\"):\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    W=tf.get_variable(\"weights\",(1,1),initializer=tf.random_normal_initializer(),dtype=tf.float32)\n",
    "    b=tf.get_variable(\"bias\",(1,),initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "    y_pred=tf.matmul(X_data,W)+b  #TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.  \n",
    "    loss=tf.reduce_sum((y_pred-y)**2)/n_samples #np.sum((y_pred-y)**2)/n_samples\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression in TensorFlow.4.Optimization: one step\n",
    "#Sample code to run 'one step' of gradient descent\n",
    "print \"start...\"\n",
    "opt=tf.train.AdamOptimizer()\n",
    "opt_operation=opt.minimize(loss) ##loss = tf.cast(loss, tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables()) \n",
    "    #TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.\n",
    "    #ValueError: Shape (1000, 1) must have rank 1\n",
    "    sess.run([opt_operation],feed_dict={X:X_data,y:y_data})\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Bad slice index Tensor(\"Const_10:0\", shape=(100,), dtype=int64) of type <class 'tensorflow.python.framework.ops.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-c7efec047687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#1.select random minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#indices_t=tf.convert_to_tensor(indices)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"X_batch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_operation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#2.run mini-batch adam(a SGD)update\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SliceHelper\u001b[1;34m(tensor, slice_spec)\u001b[0m\n\u001b[0;32m    151\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ellipsis is not currently supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bad slice index %s of type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m   \u001b[0msliced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msqueeze_dims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Bad slice index Tensor(\"Const_10:0\", shape=(100,), dtype=int64) of type <class 'tensorflow.python.framework.ops.Tensor'>"
     ]
    }
   ],
   "source": [
    "#Linear Regression in TensorFlow.4.Optimization\n",
    "#Sample code to run 'full' gradient descent\n",
    "print \"start...\"\n",
    "batch_size=100\n",
    "opt_operation=tf.train.AdamOptimizer().minimize(loss) #use adam(a SGD) to minize loss\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #Gradient descent loop for 500 steps\n",
    "    for _ in range(500):\n",
    "        indices=np.random.choice(n_samples,batch_size)  #1.select random minibatch\n",
    "        #indices_t=tf.convert_to_tensor(indices)\n",
    "        X_batch,y_batch=X_data[indices_t,:],y_data[indices_t,:]\n",
    "        print \"X_batch:\",X_batch.eval()\n",
    "        _,loss_val=sess.run([opt_operation],feed_dict={X:X_batch,y:y_batch}) #2.run mini-batch adam(a SGD)update\n",
    "        print \"loss:\",loss\n",
    "#TypeError: Bad slice index Tensor(\"Const_6:0\", shape=(100,), dtype=int64) of \n",
    "#type <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[-126.36969415 -111.04573566   79.00518716]\n",
      " [  59.8617107    66.79666063   76.64539713]\n",
      " [  41.27882623   78.11916296 -151.69552089]\n",
      " [ -40.12372603   69.69516752  187.48213565]\n",
      " [-129.57801748   14.87440946   48.5348157 ]]\n",
      "x_: [[  6.40947302e-090   2.89689622e-083   1.00000000e+000]\n",
      " [  5.13942622e-008   5.28110890e-005   9.99947138e-001]\n",
      " [  1.00102529e-016   1.00000000e+000   1.55865840e-100]\n",
      " [  1.41915627e-099   7.01083423e-052   1.00000000e+000]\n",
      " [  4.43179225e-078   2.40697350e-015   1.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "#Below is Optional Part. you can ignore it if you are in a hurry\n",
    "#Sofmtax in numpy\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Compute the softmax function in tensorflow.\n",
    "\n",
    "  You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "  not need to use all of these functions). Recall also that many common\n",
    "  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "  if x and y are both tensors). Make sure to implement the numerical stability\n",
    "  fixes as in the previous homework!\n",
    "\n",
    "  Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "  Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "  \"\"\"\n",
    "\n",
    "  ### YOUR CODE HERE\n",
    "  max_eachrow=np.max(x,axis=1,keepdims=True)\n",
    "  x=x-max_eachrow\n",
    "  probs=np.exp(x)\n",
    "  sum_eachrow=np.sum(probs,axis=1,keepdims=True)\n",
    "  probs=probs/sum_eachrow\n",
    "  out=probs\n",
    "  ### END YOUR CODE\n",
    "  \n",
    "  return out \n",
    "x=np.random.randn(5,3)*100+10\n",
    "print \"x:\",x\n",
    "x_=softmax(x)\n",
    "print \"x_:\",x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[ -86.30383715   24.90825023   21.66614383]\n",
      " [ -16.43482024  216.30176755  168.78351547]\n",
      " [  10.49246979  -28.63764785  -21.50471918]\n",
      " [ 183.73201432   41.06096441  108.0535273 ]\n",
      " [ 212.41481792   61.87556623  106.43011386]]\n",
      "x_: [[  4.83675984e-049   9.62388429e-001   3.76115712e-002]\n",
      " [  8.39042939e-102   1.00000000e+000   2.30720048e-021]\n",
      " [  1.00000000e+000   1.01392501e-017   1.26998150e-014]\n",
      " [  1.00000000e+000   1.09332756e-062   1.35909774e-033]\n",
      " [  1.00000000e+000   4.18440471e-066   9.36327675e-047]]\n",
      "x2: [[  4.83675984e-049   9.62388429e-001   3.76115712e-002]\n",
      " [  8.39042939e-102   1.00000000e+000   2.30720048e-021]\n",
      " [  1.00000000e+000   1.01392501e-017   1.26998150e-014]\n",
      " [  1.00000000e+000   1.09332756e-062   1.35909774e-033]\n",
      " [  1.00000000e+000   4.18440471e-066   9.36327675e-047]]\n"
     ]
    }
   ],
   "source": [
    "#Sofmtax in TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Compute the softmax function in tensorflow.\n",
    "\n",
    "  You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "  not need to use all of these functions). Recall also that many common\n",
    "  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "  if x and y are both tensors). Make sure to implement the numerical stability\n",
    "  fixes as in the previous homework!\n",
    "\n",
    "  Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "  Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "  \"\"\"\n",
    "  ### YOUR CODE HERE \n",
    "  max_eachrow=tf.reduce_max(x,reduction_indices=[1]) #max_eachrow=np.max(x,axis=1,keepdims=True)\n",
    "  max_eachrow_=tf.expand_dims(max_eachrow,1)                    \n",
    "  x=x-max_eachrow_ #a2=tf.reshape(a,(1,4)).eval()\n",
    "  probs=tf.exp(x) #probs=np.exp(x)\n",
    "  sum_eachrow=tf.reduce_sum(probs,reduction_indices=[1]) #sum_eachrow=np.sum(probs,axis=1,keepdims=True)\n",
    "  sum_eachrow=tf.expand_dims(sum_eachrow,1)\n",
    "  probs=probs/sum_eachrow\n",
    "  out=probs\n",
    "  ### END YOUR CODE \n",
    "  return out \n",
    "\n",
    "x=np.random.randn(5,3)*100+10\n",
    "x2=tf.convert_to_tensor(x)\n",
    "print \"x:\",x\n",
    "sess=tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "x_=softmax(x2)\n",
    "print \"x_:\",x_.eval()\n",
    "\n",
    "x2=tf.nn.softmax(x2)\n",
    "print \"x2:\",x2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f2a7f030ad0>> ignored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Under-sharing: Variable y does not exist, disallowed. Did you mean to set reuse=None in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d13c4d7607a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#tf.get_variable_scope().reuse_variables()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yhat\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, collections)\u001b[0m\n\u001b[0;32m    254\u001b[0m   return get_variable_scope().get_variable(_get_default_variable_store(), name,\n\u001b[0;32m    255\u001b[0m                                            \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m                                            trainable, collections)\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, trainable, collections)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m       return var_store.get_variable(full_name, shape, dtype, initializer,\n\u001b[1;32m--> 188\u001b[1;33m                                     self.reuse, trainable, collections)\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, reuse, trainable, collections)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_check\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m       raise ValueError(\"Under-sharing: Variable %s does not exist, disallowed.\"\n\u001b[1;32m--> 118\u001b[1;33m                        \" Did you mean to set reuse=None in VarScope?\" % name)\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[1;31mValueError\u001b[0m: Under-sharing: Variable y does not exist, disallowed. Did you mean to set reuse=None in VarScope?"
     ]
    }
   ],
   "source": [
    "#cross entropy loss using basic function of tensorflow.\n",
    "def cross_entropy_loss(y, yhat):\n",
    "  \"\"\"\n",
    "  Compute the cross entropy loss in tensorflow.\n",
    "\n",
    "  y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "  of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "  be of dtype tf.float32.\n",
    "\n",
    "  The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "  solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "  Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "        functions.\n",
    "\n",
    "  Args:\n",
    "    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "          probability distribution and should sum to 1.\n",
    "  Returns:\n",
    "    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "          tensor in the problem.\n",
    "  \"\"\"\n",
    "  ### YOUR CODE HERE\n",
    "  y=tf.to_float(y)\n",
    "  yhat=tf.to_float(yhat)\n",
    "  #1.get log of yhat\n",
    "  yhat_log=tf.log(yhat)\n",
    "  #2.multiply \n",
    "  mul=-1.0*tf.mul(y,yhat_log)\n",
    "  out=tf.reduce_sum(mul)\n",
    "  ### END YOUR CODE\n",
    "  return out\n",
    "\n",
    "#test the function\n",
    "sess=tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "y=tf.get_variable(\"y\",(5,1),initializer=tf.random_normal_initializer(),dtype=tf.float32 )\n",
    "yhat=tf.get_variable(\"yhat\",(5,1),initializer=tf.random_normal_initializer(),dtype=tf.float32 )\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print \"y:\",y.eval()\n",
    "print \"yhat:\",yhat.eval()\n",
    "cross_entropy_loss(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started. \n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  print \"Running basic tests...\"\n",
    "  test1 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test1 = test1.eval()\n",
    "  assert np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "  test2 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[-1001,-1002]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test2 = test2.eval()\n",
    "  assert np.amax(np.fabs(test2 - np.array(\n",
    "      [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "  print \"Basic (non-exhaustive) softmax tests pass\\n\"\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started.\n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "  yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "  test1 = cross_entropy_loss(\n",
    "      tf.convert_to_tensor(y, dtype=tf.int32),\n",
    "      tf.convert_to_tensor(yhat, dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "    test1 = test1.eval()\n",
    "  result = -3 * np.log(.5)\n",
    "  assert np.amax(np.fabs(test1 - result)) <= 1e-6\n",
    "  print \"Basic (non-exhaustive) cross-entropy tests pass\\n\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  test_softmax_basic()\n",
    "  test_cross_entropy_loss_basic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f7b2c0d881b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'autoreload 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mq1_classifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_SoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "print \"start...\"\n",
    "import tensorflow as tf\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from q1_classifier import SoftmaxModel\n",
    "t=SoftmaxModel()\n",
    "t.test_SoftmaxModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.input_labels: (1024,)\n",
      "pred: (64, 5)\n",
      "Epoch 0: loss = 63.15 (0.100 sec)\n",
      "Epoch 1: loss = 21.69 (0.069 sec)\n",
      "Epoch 2: loss = 11.66 (0.061 sec)\n",
      "Epoch 3: loss = 7.79 (0.065 sec)\n",
      "Epoch 4: loss = 5.80 (0.062 sec)\n",
      "Epoch 5: loss = 4.61 (0.069 sec)\n",
      "Epoch 6: loss = 3.82 (0.063 sec)\n",
      "Epoch 7: loss = 3.25 (0.063 sec)\n",
      "Epoch 8: loss = 2.83 (0.065 sec)\n",
      "Epoch 9: loss = 2.51 (0.063 sec)\n",
      "Epoch 10: loss = 2.25 (0.066 sec)\n",
      "Epoch 11: loss = 2.04 (0.066 sec)\n",
      "Epoch 12: loss = 1.86 (0.069 sec)\n",
      "Epoch 13: loss = 1.71 (0.079 sec)\n",
      "Epoch 14: loss = 1.59 (0.081 sec)\n",
      "Epoch 15: loss = 1.48 (0.079 sec)\n",
      "Epoch 16: loss = 1.38 (0.068 sec)\n",
      "Epoch 17: loss = 1.30 (0.063 sec)\n",
      "Epoch 18: loss = 1.23 (0.055 sec)\n",
      "Epoch 19: loss = 1.16 (0.055 sec)\n",
      "Epoch 20: loss = 1.10 (0.054 sec)\n",
      "Epoch 21: loss = 1.05 (0.055 sec)\n",
      "Epoch 22: loss = 1.00 (0.059 sec)\n",
      "Epoch 23: loss = 0.95 (0.058 sec)\n",
      "Epoch 24: loss = 0.91 (0.056 sec)\n",
      "Epoch 25: loss = 0.88 (0.056 sec)\n",
      "Epoch 26: loss = 0.84 (0.055 sec)\n",
      "Epoch 27: loss = 0.81 (0.058 sec)\n",
      "Epoch 28: loss = 0.78 (0.055 sec)\n",
      "Epoch 29: loss = 0.75 (0.057 sec)\n",
      "Epoch 30: loss = 0.73 (0.056 sec)\n",
      "Epoch 31: loss = 0.70 (0.056 sec)\n",
      "Epoch 32: loss = 0.68 (0.054 sec)\n",
      "Epoch 33: loss = 0.66 (0.073 sec)\n",
      "Epoch 34: loss = 0.64 (0.066 sec)\n",
      "Epoch 35: loss = 0.62 (0.066 sec)\n",
      "Epoch 36: loss = 0.60 (0.068 sec)\n",
      "Epoch 37: loss = 0.59 (0.064 sec)\n",
      "Epoch 38: loss = 0.57 (0.060 sec)\n",
      "Epoch 39: loss = 0.56 (0.056 sec)\n",
      "Epoch 40: loss = 0.54 (0.055 sec)\n",
      "Epoch 41: loss = 0.53 (0.057 sec)\n",
      "Epoch 42: loss = 0.52 (0.056 sec)\n",
      "Epoch 43: loss = 0.50 (0.057 sec)\n",
      "Epoch 44: loss = 0.49 (0.057 sec)\n",
      "Epoch 45: loss = 0.48 (0.058 sec)\n",
      "Epoch 46: loss = 0.47 (0.058 sec)\n",
      "Epoch 47: loss = 0.46 (0.070 sec)\n",
      "Epoch 48: loss = 0.45 (0.067 sec)\n",
      "Epoch 49: loss = 0.44 (0.069 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_classifier import SoftmaxModel\n",
    "from q1_classifier import Config\n",
    "def test_SoftmaxModel():\n",
    "  \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "  config = Config()\n",
    "  with tf.Graph().as_default():\n",
    "    model = SoftmaxModel(config)\n",
    "  \n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "  \n",
    "    # Run the Op to initialize the variables.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "  \n",
    "    losses = model.fit(sess, model.input_data, model.input_labels)\n",
    "\n",
    "  # If ops are implemented correctly, the average loss should fall close to zero\n",
    "  # rapidly.\n",
    "  assert losses[-1] < .5\n",
    "  print \"Basic (non-exhaustive) classifier tests pass\\n\"\n",
    "\n",
    "test_SoftmaxModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <built-in function sum>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd5eeb30d2f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mq2_initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mq2_initialization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_initialization_basic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/assignments/q2_initialization.py\u001b[0m in \u001b[0;36mtest_initialization_basic\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m   \u001b[0mxavier_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxavier_weight_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m   \u001b[0mxavier_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0mxavier_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/assignments/q2_initialization.py\u001b[0m in \u001b[0;36m_xavier_initializer\u001b[1;34m(shape, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mbound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m### END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36msqrt\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m   \"\"\"\n\u001b[1;32m-> 1413\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sqrt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    409\u001b[0m             values = ops.convert_to_tensor(\n\u001b[0;32m    410\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m                 as_ref=input_arg.is_ref)\n\u001b[0m\u001b[0;32m    412\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconversion_func\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfuncs_at_priority\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m           raise RuntimeError(\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    176\u001b[0m                                          as_ref=False):\n\u001b[0;32m    177\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    159\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 161\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n\u001b[0m\u001b[0;32m    162\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape)\u001b[0m\n\u001b[0;32m    372\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got <built-in function sum>"
     ]
    }
   ],
   "source": [
    "import q2_initialization\n",
    "q2_initialization.test_initialization_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "print \"start...\"\n",
    "from q2_initialization import xavier_weight_init\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '_xavier_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8f71f9b2b870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxavier_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxavier_weight_init\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mxavier_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Basic (non-exhaustive) Xavier initialization tests pass\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute '_xavier_initializer'"
     ]
    }
   ],
   "source": [
    "shape = (1, 2, 3)\n",
    "xavier_mat = xavier_weight_init._xavier_initializer(shape)\n",
    "assert xavier_mat.get_shape() == shape\n",
    "print \"Basic (non-exhaustive) Xavier initialization tests pass\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "1\n",
      "2\n",
      "Epoch 0\n",
      "Training loss: 115.819282532\n",
      "Training acc: 0.2099609375\n",
      "Validation loss: 115.251800537\n",
      "\n",
      "[[158 128 140 159 219]\n",
      " [ 18   7   5  10  15]\n",
      " [  1   2   1   3   4]\n",
      " [ 16  10  13  23  15]\n",
      " [ 17  14  11  13  22]]\n",
      "Tag: O - P 0.7524 / R 0.1965\n",
      "Tag: LOC - P 0.0435 / R 0.1273\n",
      "Tag: MISC - P 0.0059 / R 0.0909\n",
      "Tag: ORG - P 0.1106 / R 0.2987\n",
      "Tag: PER - P 0.0800 / R 0.2857\n",
      "Total time: 3.56954598427\n",
      "Epoch 1\n",
      "Training loss: 115.733306885\n",
      "Training acc: 0.203125\n",
      "Validation loss: 114.67073822\n",
      "\n",
      "[[161 146 137 152 208]\n",
      " [ 16   4  11  11  13]\n",
      " [  1   1   1   2   6]\n",
      " [ 13  14  14  22  14]\n",
      " [ 13  13   9  11  31]]\n",
      "Tag: O - P 0.7892 / R 0.2002\n",
      "Tag: LOC - P 0.0225 / R 0.0727\n",
      "Tag: MISC - P 0.0058 / R 0.0909\n",
      "Tag: ORG - P 0.1111 / R 0.2857\n",
      "Tag: PER - P 0.1140 / R 0.4026\n",
      "Total time: 3.9225769043\n",
      "Epoch 2\n",
      "Training loss: 115.833480835\n",
      "Training acc: 0.1982421875\n",
      "Validation loss: 114.125106812\n",
      "\n",
      "[[185 145 116 155 203]\n",
      " [ 16   5   7  11  16]\n",
      " [  3   4   0   2   2]\n",
      " [ 21   9  12  21  14]\n",
      " [ 16  19   9  13  20]]\n",
      "Tag: O - P 0.7676 / R 0.2301\n",
      "Tag: LOC - P 0.0275 / R 0.0909\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.1040 / R 0.2727\n",
      "Tag: PER - P 0.0784 / R 0.2597\n",
      "Total time: 3.91627693176\n",
      "Epoch 3\n",
      "Training loss: 114.939750671\n",
      "Training acc: 0.216796875\n",
      "Validation loss: 112.156646729\n",
      "\n",
      "[[204 133 142 132 193]\n",
      " [ 13  10   4  10  18]\n",
      " [  2   3   2   2   2]\n",
      " [ 13  10  12  24  18]\n",
      " [ 17  16   7  16  21]]\n",
      "Tag: O - P 0.8193 / R 0.2537\n",
      "Tag: LOC - P 0.0581 / R 0.1818\n",
      "Tag: MISC - P 0.0120 / R 0.1818\n",
      "Tag: ORG - P 0.1304 / R 0.3117\n",
      "Tag: PER - P 0.0833 / R 0.2727\n",
      "Total time: 3.77646207809\n",
      "Epoch 4\n",
      "Training loss: 114.432403564\n",
      "Training acc: 0.2197265625\n",
      "Validation loss: 112.730926514\n",
      "\n",
      "[[200 131 126 144 203]\n",
      " [ 14   6   9  12  14]\n",
      " [  3   3   0   2   3]\n",
      " [ 15  10  14  18  20]\n",
      " [ 14  14  11  12  26]]\n",
      "Tag: O - P 0.8130 / R 0.2488\n",
      "Tag: LOC - P 0.0366 / R 0.1091\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.0957 / R 0.2338\n",
      "Tag: PER - P 0.0977 / R 0.3377\n",
      "Total time: 1.38295292854\n",
      "Epoch 5\n",
      "Training loss: 113.559448242\n",
      "Training acc: 0.240234375\n",
      "Validation loss: 112.075958252\n",
      "\n",
      "[[213 133  97 144 217]\n",
      " [ 20   5   4   9  17]\n",
      " [  4   2   0   4   1]\n",
      " [ 18  10   9  21  19]\n",
      " [ 14  13  11  17  22]]\n",
      "Tag: O - P 0.7918 / R 0.2649\n",
      "Tag: LOC - P 0.0307 / R 0.0909\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.1077 / R 0.2727\n",
      "Tag: PER - P 0.0797 / R 0.2857\n",
      "Total time: 3.67810106277\n",
      "Epoch 6\n",
      "Training loss: 113.02281189\n",
      "Training acc: 0.2431640625\n",
      "Validation loss: 112.616195679\n",
      "\n",
      "[[206 124 145 141 188]\n",
      " [ 20   7   5   8  15]\n",
      " [  1   0   3   2   5]\n",
      " [ 20   6  13  18  20]\n",
      " [ 21  14  12  13  17]]\n",
      "Tag: O - P 0.7687 / R 0.2562\n",
      "Tag: LOC - P 0.0464 / R 0.1273\n",
      "Tag: MISC - P 0.0169 / R 0.2727\n",
      "Tag: ORG - P 0.0989 / R 0.2338\n",
      "Tag: PER - P 0.0694 / R 0.2208\n",
      "Total time: 1.46320796013\n",
      "Epoch 7\n",
      "Training loss: 112.796783447\n",
      "Training acc: 0.2509765625\n",
      "Validation loss: 112.228118896\n",
      "\n",
      "[[218 137 109 147 193]\n",
      " [ 21   5   4   9  16]\n",
      " [  2   3   0   4   2]\n",
      " [ 18  13  15  19  12]\n",
      " [ 14  14  11  13  25]]\n",
      "Tag: O - P 0.7985 / R 0.2711\n",
      "Tag: LOC - P 0.0291 / R 0.0909\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.0990 / R 0.2468\n",
      "Tag: PER - P 0.1008 / R 0.3247\n",
      "Total time: 1.54810595512\n",
      "Epoch 8\n",
      "Training loss: 112.909500122\n",
      "Training acc: 0.24609375\n",
      "Validation loss: 110.891403198\n",
      "\n",
      "[[236 131 105 160 172]\n",
      " [ 22   2   5   8  18]\n",
      " [  3   4   1   0   3]\n",
      " [ 23  10  11  19  14]\n",
      " [ 21  16  12   4  24]]\n",
      "Tag: O - P 0.7738 / R 0.2935\n",
      "Tag: LOC - P 0.0123 / R 0.0364\n",
      "Tag: MISC - P 0.0075 / R 0.0909\n",
      "Tag: ORG - P 0.0995 / R 0.2468\n",
      "Tag: PER - P 0.1039 / R 0.3117\n",
      "Total time: 4.11731505394\n",
      "Epoch 9\n",
      "Training loss: 111.853485107\n",
      "Training acc: 0.26171875\n",
      "Validation loss: 110.798797607\n",
      "\n",
      "[[235 108 104 144 213]\n",
      " [ 18   4   8  12  13]\n",
      " [  3   2   2   2   2]\n",
      " [ 26  11  10  19  11]\n",
      " [ 24  14   7  12  20]]\n",
      "Tag: O - P 0.7680 / R 0.2923\n",
      "Tag: LOC - P 0.0288 / R 0.0727\n",
      "Tag: MISC - P 0.0153 / R 0.1818\n",
      "Tag: ORG - P 0.1005 / R 0.2468\n",
      "Tag: PER - P 0.0772 / R 0.2597\n",
      "Total time: 4.04427409172\n",
      "Epoch 10\n",
      "Training loss: 109.898780823\n",
      "Training acc: 0.2958984375\n",
      "Validation loss: 109.474151611\n",
      "\n",
      "[[259 125  87 145 188]\n",
      " [ 21   5   5   8  16]\n",
      " [  2   2   2   2   3]\n",
      " [ 24  11   9  20  13]\n",
      " [ 20  13  12  13  19]]\n",
      "Tag: O - P 0.7945 / R 0.3221\n",
      "Tag: LOC - P 0.0321 / R 0.0909\n",
      "Tag: MISC - P 0.0174 / R 0.1818\n",
      "Tag: ORG - P 0.1064 / R 0.2597\n",
      "Tag: PER - P 0.0795 / R 0.2468\n",
      "Total time: 4.64587688446\n",
      "Epoch 11\n",
      "Training loss: 109.385543823\n",
      "Training acc: 0.302734375\n",
      "Validation loss: 108.330444336\n",
      "\n",
      "[[277  98  90 151 188]\n",
      " [ 18   6   6   7  18]\n",
      " [  3   3   0   0   5]\n",
      " [ 22  10   8  20  17]\n",
      " [ 23  16   5  13  20]]\n",
      "Tag: O - P 0.8076 / R 0.3445\n",
      "Tag: LOC - P 0.0451 / R 0.1091\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.1047 / R 0.2597\n",
      "Tag: PER - P 0.0806 / R 0.2597\n",
      "Total time: 3.92697811127\n",
      "Epoch 12\n",
      "Training loss: 108.930755615\n",
      "Training acc: 0.30859375\n",
      "Validation loss: 110.035835266\n",
      "\n",
      "[[247 127 113 124 193]\n",
      " [ 21   4   6   6  18]\n",
      " [  4   1   2   3   1]\n",
      " [ 18  12  10  19  18]\n",
      " [ 24  12   9  12  20]]\n",
      "Tag: O - P 0.7866 / R 0.3072\n",
      "Tag: LOC - P 0.0256 / R 0.0727\n",
      "Tag: MISC - P 0.0143 / R 0.1818\n",
      "Tag: ORG - P 0.1159 / R 0.2468\n",
      "Tag: PER - P 0.0800 / R 0.2597\n",
      "Total time: 1.37056088448\n",
      "Epoch 13\n",
      "Training loss: 109.359138489\n",
      "Training acc: 0.302734375\n",
      "Validation loss: 108.264457703\n",
      "\n",
      "[[285 107 100 133 179]\n",
      " [ 22   9   7   4  13]\n",
      " [  2   1   1   3   4]\n",
      " [ 23  14  12  18  10]\n",
      " [ 23   9  10  16  19]]\n",
      "Tag: O - P 0.8028 / R 0.3545\n",
      "Tag: LOC - P 0.0643 / R 0.1636\n",
      "Tag: MISC - P 0.0077 / R 0.0909\n",
      "Tag: ORG - P 0.1034 / R 0.2338\n",
      "Tag: PER - P 0.0844 / R 0.2468\n",
      "Total time: 3.34985399246\n",
      "Epoch 14\n",
      "Training loss: 107.003593445\n",
      "Training acc: 0.3408203125\n",
      "Validation loss: 107.753425598\n",
      "\n",
      "[[288 110  82 142 182]\n",
      " [ 22   3   6   8  16]\n",
      " [  4   2   0   2   3]\n",
      " [ 24   7   7  27  12]\n",
      " [ 30   6   8  15  18]]\n",
      "Tag: O - P 0.7826 / R 0.3582\n",
      "Tag: LOC - P 0.0234 / R 0.0545\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.1392 / R 0.3506\n",
      "Tag: PER - P 0.0779 / R 0.2338\n",
      "Total time: 3.33874082565\n",
      "Epoch 15\n",
      "Training loss: 107.9765625\n",
      "Training acc: 0.3212890625\n",
      "Validation loss: 107.102729797\n",
      "\n",
      "[[305 102  87 132 178]\n",
      " [ 27   4   5   7  12]\n",
      " [  3   1   2   2   3]\n",
      " [ 23   7   8  19  20]\n",
      " [ 28   9  11   9  20]]\n",
      "Tag: O - P 0.7902 / R 0.3794\n",
      "Tag: LOC - P 0.0325 / R 0.0727\n",
      "Tag: MISC - P 0.0177 / R 0.1818\n",
      "Tag: ORG - P 0.1124 / R 0.2468\n",
      "Tag: PER - P 0.0858 / R 0.2597\n",
      "Total time: 3.8519179821\n",
      "Epoch 16\n",
      "Training loss: 107.013755798\n",
      "Training acc: 0.33984375\n",
      "Validation loss: 107.21282959\n",
      "\n",
      "[[299  99  93 134 179]\n",
      " [ 26   3   4   7  15]\n",
      " [  3   2   1   2   3]\n",
      " [ 23   9  14  16  15]\n",
      " [ 20  12  12  14  19]]\n",
      "Tag: O - P 0.8059 / R 0.3719\n",
      "Tag: LOC - P 0.0240 / R 0.0545\n",
      "Tag: MISC - P 0.0081 / R 0.0909\n",
      "Tag: ORG - P 0.0925 / R 0.2078\n",
      "Tag: PER - P 0.0823 / R 0.2468\n",
      "Total time: 1.07771611214\n",
      "Epoch 17\n",
      "Training loss: 105.81867981\n",
      "Training acc: 0.3603515625\n",
      "Validation loss: 106.524795532\n",
      "\n",
      "[[307 100  98 117 182]\n",
      " [ 17   6   8  12  12]\n",
      " [  1   3   3   1   3]\n",
      " [ 24  14  11  20   8]\n",
      " [ 24  12  13   9  19]]\n",
      "Tag: O - P 0.8231 / R 0.3818\n",
      "Tag: LOC - P 0.0444 / R 0.1091\n",
      "Tag: MISC - P 0.0226 / R 0.2727\n",
      "Tag: ORG - P 0.1258 / R 0.2597\n",
      "Tag: PER - P 0.0848 / R 0.2468\n",
      "Total time: 3.34141802788\n",
      "Epoch 18\n",
      "Training loss: 105.513374329\n",
      "Training acc: 0.36328125\n",
      "Validation loss: 106.028396606\n",
      "\n",
      "[[311  99  90 135 169]\n",
      " [ 28   6   5   7   9]\n",
      " [  1   4   2   3   1]\n",
      " [ 27   9  10  21  10]\n",
      " [ 25   7  11  12  22]]\n",
      "Tag: O - P 0.7934 / R 0.3868\n",
      "Tag: LOC - P 0.0480 / R 0.1091\n",
      "Tag: MISC - P 0.0169 / R 0.1818\n",
      "Tag: ORG - P 0.1180 / R 0.2727\n",
      "Tag: PER - P 0.1043 / R 0.2857\n",
      "Total time: 3.49990701675\n",
      "Epoch 19\n",
      "Training loss: 105.732933044\n",
      "Training acc: 0.359375\n",
      "Validation loss: 105.323287964\n",
      "\n",
      "[[333 104  94 121 152]\n",
      " [ 27   2   2   9  15]\n",
      " [  3   3   2   2   1]\n",
      " [ 27  12   8  15  15]\n",
      " [ 28  10   6  10  23]]\n",
      "Tag: O - P 0.7967 / R 0.4142\n",
      "Tag: LOC - P 0.0153 / R 0.0364\n",
      "Tag: MISC - P 0.0179 / R 0.1818\n",
      "Tag: ORG - P 0.0955 / R 0.1948\n",
      "Tag: PER - P 0.1117 / R 0.2987\n",
      "Total time: 3.39809799194\n",
      "Epoch 20\n",
      "Training loss: 103.817520142\n",
      "Training acc: 0.3916015625\n",
      "Validation loss: 105.402961731\n",
      "\n",
      "[[337  93  88 129 157]\n",
      " [ 26   5   3   9  12]\n",
      " [  3   3   0   2   3]\n",
      " [ 29  10   9  17  12]\n",
      " [ 30  12   8   9  18]]\n",
      "Tag: O - P 0.7929 / R 0.4192\n",
      "Tag: LOC - P 0.0407 / R 0.0909\n",
      "Tag: MISC - P 0.0000 / R 0.0000\n",
      "Tag: ORG - P 0.1024 / R 0.2208\n",
      "Tag: PER - P 0.0891 / R 0.2338\n",
      "Total time: 1.15892601013\n",
      "Epoch 21\n",
      "Training loss: 104.726112366\n",
      "Training acc: 0.373046875\n",
      "Validation loss: 105.65146637\n",
      "\n",
      "[[323 104  88 130 159]\n",
      " [ 27   7   1   8  12]\n",
      " [  5   1   2   0   3]\n",
      " [ 28   6  12  16  15]\n",
      " [ 26   8  11  12  20]]\n",
      "Tag: O - P 0.7897 / R 0.4017\n",
      "Tag: LOC - P 0.0556 / R 0.1273\n",
      "Tag: MISC - P 0.0175 / R 0.1818\n",
      "Tag: ORG - P 0.0964 / R 0.2078\n",
      "Tag: PER - P 0.0957 / R 0.2597\n",
      "Total time: 1.06200909615\n",
      "Epoch 22\n",
      "Training loss: 103.107223511\n",
      "Training acc: 0.396484375\n",
      "Validation loss: 104.934112549\n",
      "\n",
      "[[347  99  96 119 143]\n",
      " [ 25   6   6   7  11]\n",
      " [  4   3   1   2   1]\n",
      " [ 32   8  12  11  14]\n",
      " [ 34  11   9   9  14]]\n",
      "Tag: O - P 0.7851 / R 0.4316\n",
      "Tag: LOC - P 0.0472 / R 0.1091\n",
      "Tag: MISC - P 0.0081 / R 0.0909\n",
      "Tag: ORG - P 0.0743 / R 0.1429\n",
      "Tag: PER - P 0.0765 / R 0.1818\n",
      "Total time: 3.39609599113\n",
      "Epoch 23\n",
      "Training loss: 102.795783997\n",
      "Training acc: 0.412109375\n",
      "Validation loss: 103.879859924\n",
      "\n",
      "[[362 101  79 112 150]\n",
      " [ 32   2   3   5  13]\n",
      " [  5   1   1   2   2]\n",
      " [ 30  11   9  14  13]\n",
      " [ 25   9  12  11  20]]\n",
      "Tag: O - P 0.7974 / R 0.4502\n",
      "Tag: LOC - P 0.0161 / R 0.0364\n",
      "Tag: MISC - P 0.0096 / R 0.0909\n",
      "Tag: ORG - P 0.0972 / R 0.1818\n",
      "Tag: PER - P 0.1010 / R 0.2597\n",
      "Total time: 3.41278100014\n",
      "Test\n",
      "=-=-=\n",
      "Writing predictions to q2_test.predicted\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "print \"start...\"\n",
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from q2_NER import Config\n",
    "print \"1\"\n",
    "from q2_NER import NERModel\n",
    "print \"2\"\n",
    "import time\n",
    "import os\n",
    "import getpass\n",
    "import sys\n",
    "import numpy as np\n",
    "#from q2_initialization import xavier_weight_init\n",
    "import data_utils.utils as du \n",
    "import data_utils.ner as ner\n",
    "from utils import data_iterator\n",
    "from model import LanguageModel\n",
    "\n",
    "def print_confusion(confusion, num_to_tag):\n",
    "    \"\"\"Helper method that prints confusion matrix.\"\"\"\n",
    "    # Summing top to bottom gets the total number of tags guessed as T\n",
    "    total_guessed_tags = confusion.sum(axis=0)\n",
    "    # Summing left to right gets the total number of true tags\n",
    "    total_true_tags = confusion.sum(axis=1)\n",
    "    print\n",
    "    print confusion\n",
    "    for i, tag in sorted(num_to_tag.items()):\n",
    "        prec = confusion[i, i] / float(total_guessed_tags[i])\n",
    "        recall = confusion[i, i] / float(total_true_tags[i])\n",
    "        print 'Tag: {} - P {:2.4f} / R {:2.4f}'.format(tag, prec, recall)\n",
    "\n",
    "def calculate_confusion(config, predicted_indices, y_indices):\n",
    "    \"\"\"Helper method that calculates confusion matrix.\"\"\"\n",
    "    confusion = np.zeros((config.label_size, config.label_size), dtype=np.int32)\n",
    "    for i in xrange(len(y_indices)):\n",
    "        correct_label = y_indices[i]\n",
    "        guessed_label = predicted_indices[i]\n",
    "        confusion[correct_label, guessed_label] += 1\n",
    "    return confusion\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "  \"\"\"Saves predictions to provided file.\"\"\"\n",
    "  with open(filename, \"wb\") as f:\n",
    "    for prediction in predictions:\n",
    "      f.write(str(prediction) + \"\\n\")\n",
    "\n",
    "def test_NER():\n",
    "  \"\"\"Test NER model implementation.\n",
    "\n",
    "  You can use this function to test your implementation of the Named Entity\n",
    "  Recognition network. When debugging, set max_epochs in the Config object to 1\n",
    "  so you can rapidly iterate.\n",
    "  \"\"\"\n",
    "  config = Config()\n",
    "  with tf.Graph().as_default():\n",
    "    model = NERModel(config)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "      best_val_loss = float('inf')\n",
    "      best_val_epoch = 0\n",
    "\n",
    "      session.run(init)\n",
    "      for epoch in xrange(config.max_epochs):\n",
    "        print 'Epoch {}'.format(epoch)\n",
    "        start = time.time()\n",
    "        ###\n",
    "        train_loss, train_acc = model.run_epoch(session, model.X_train,\n",
    "                                                model.y_train)\n",
    "        val_loss, predictions = model.predict(session, model.X_dev, model.y_dev)\n",
    "        print 'Training loss: {}'.format(train_loss)\n",
    "        print 'Training acc: {}'.format(train_acc)\n",
    "        print 'Validation loss: {}'.format(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          best_val_epoch = epoch\n",
    "          if not os.path.exists(\"./weights\"):\n",
    "            os.makedirs(\"./weights\")\n",
    "        \n",
    "          saver.save(session, './weights/ner.weights')\n",
    "        if epoch - best_val_epoch > config.early_stopping:\n",
    "          break\n",
    "        ###\n",
    "        confusion = calculate_confusion(config, predictions, model.y_dev)\n",
    "        print_confusion(confusion, model.num_to_tag)\n",
    "        print 'Total time: {}'.format(time.time() - start)\n",
    "      \n",
    "      saver.restore(session, './weights/ner.weights')\n",
    "      print 'Test'\n",
    "      print '=-=-='\n",
    "      print 'Writing predictions to q2_test.predicted'\n",
    "      _, predictions = model.predict(session, model.X_test, model.y_test)\n",
    "      save_predictions(predictions, \"q2_test.predicted\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  test_NER()\n",
    "print \"end...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

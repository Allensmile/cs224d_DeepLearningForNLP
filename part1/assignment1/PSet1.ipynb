{
 "metadata": {
  "name": "",
  "signature": "sha256:50122b4accbae59885981050be89bc38f6d6ede64060637156ff30fbcde50f24"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "def softmax(x):\n",
      "    \"\"\"\n",
      "    Compute the softmax function for each row of the input x.\n",
      "\n",
      "    It is crucial that this function is optimized for speed because\n",
      "    it will be used frequently in later code.\n",
      "    You might find numpy functions np.exp, np.sum, np.reshape,\n",
      "    np.max, and numpy broadcasting useful for this task. (numpy\n",
      "    broadcasting documentation:\n",
      "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      "\n",
      "    You should also make sure that your code works for one\n",
      "    dimensional inputs (treat the vector as a row), you might find\n",
      "    it helpful for your later problems.\n",
      "\n",
      "    You must implement the optimization in problem 1(a) of the \n",
      "    written assignment!\n",
      "    \"\"\"\n",
      "    #N=x.shape[0]\n",
      "    ##################### YOUR CODE HERE ##################################\n",
      "    D=x.shape\n",
      "    if len(D)==2:\n",
      "        probs = np.exp(x - np.max(x, axis=1, keepdims=True)) #max operation according to row: means for each row, get a max value.\n",
      "        probs /= np.sum(probs, axis=1, keepdims=True)        #sum operation according to row: means for each row, get a sum value\n",
      "    elif len(D)==1: #one dimension\n",
      "        print \"one dimension.\"\n",
      "        max=np.max(x)\n",
      "        newx=x-max\n",
      "        newx_exp=np.exp(newx)\n",
      "        sum=np.sum(newx_exp)\n",
      "        probs=newx_exp/sum  \n",
      "    #raise NotImplementedError\n",
      "    x=probs\n",
      "    #################### END YOUR CODE #####################################\n",
      "    #print \"x:\",x\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_softmax_basic3():\n",
      "    \"\"\"\n",
      "    Some simple tests to get you started. \n",
      "    Warning: these are not exhaustive.\n",
      "    \"\"\"\n",
      "   \n",
      "    test3 = softmax(np.array([[-1001,-1002]]))\n",
      "    print \"test3.shape:\",test3.shape\n",
      "    print test3\n",
      "    assert np.amax(np.fabs(test3 - np.array(\n",
      "        [0.73105858, 0.26894142]))) <= 1e-6\n",
      "\n",
      "    print \"You should verify these results!\\n\"\n",
      "test_softmax_basic3()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test3.shape: (1L, 2L)\n",
        "[[ 0.73105858  0.26894142]]\n",
        "You should verify these results!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_softmax_basic1():\n",
      "    \"\"\"\n",
      "    Some simple tests to get you started. \n",
      "    Warning: these are not exhaustive.\n",
      "    \"\"\"\n",
      "    print \"Running basic tests...\"\n",
      "    test1 = softmax(np.array([1,2]))\n",
      "    print \"test1:\",test1\n",
      "    assert np.amax(np.fabs(test1 - np.array(\n",
      "       [0.26894142,  0.73105858]))) <= 1e-6\n",
      "test_softmax_basic1() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running basic tests...\n",
        "one dimension.\n",
        "test1: [ 0.26894142  0.73105858]\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_softmax_basic2():\n",
      "    \"\"\"\n",
      "    Some simple tests to get you started. \n",
      "    Warning: these are not exhaustive.\n",
      "    \"\"\"\n",
      "    print \"Running basic tests...\"\n",
      "    \n",
      "\n",
      "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
      "    print test2\n",
      "    assert np.amax(np.fabs(test2 - np.array(\n",
      "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
      "test_softmax_basic2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running basic tests...\n",
        "[[ 0.26894142  0.73105858]\n",
        " [ 0.26894142  0.73105858]]\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}